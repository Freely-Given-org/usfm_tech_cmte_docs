#!/usr/bin/env -S python3 -u

import argparse, zipfile, os, sys, itertools, re
from multiprocessing import Pool, get_context, current_process, set_start_method
from math import sqrt
import logging
import logging.config

def lognprint(s, q, level=logging.DEBUG):
    log.log(level, s)
    if not q:
        print(s, flush=True)

try:
    from usfmtc import USX, _grammarDoc, _usfmGrammar
except ImportError:
    sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'lib'))
    from usfmtc import USX, _grammarDoc, _usfmGrammar
from usfmtc.parser import NoParseError, Parser
from usfmtc.usxmodel import addesids, etCmp, cleanup, partypes
import xml.etree.ElementTree as et
from lxml import etree

def proconezip(fpath, sfmp, log, relax, args):
    lognprint(f"{fpath}: started", args.quiet)
    try:
        zipf = zipfile.ZipFile(fpath)
    except (OSError, zipfile.BadZipFile) as e:
        lognprint(f"{fpath}: Skipped {e}", args.quiet, logging.INFO)
        return (fpath, [])
    try:
        fh = zipf.open("source/source.zip")
    except (KeyError, OSError) as e:
        zipf.close()
        lognprint(f"{fpath}: Skipped (no source) {e}", args.quiet, logging.INFO)
        return (fpath, [])
    try:
        szip = zipfile.ZipFile(fh)
    except zipfile.BadZipFile as e:
        zipf.close()
        lognprint(f"{fpath}: Skipped. Bad Zip file source/source.zip {e}", args.quiet, logging.INFO)
        return (fpath, [])
    results = []
    anyfailed = False
    bookcount = 0
    try:
        if args.do in ("all", "usfm"):
            for f in szip.namelist():
                if f.lower().endswith('sfm'):
                    m = re.match(r'(\d{2})', f)
                    if m is None or not (0 < int(m.group(1)) < 68):
                        continue
                else:
                    continue
                if args.verbose:
                    lognprint(f"{fpath}/{f}", args.quiet)
                sfmfh = szip.open(f)
                sfmdat = sfmfh.read().decode("utf-8", errors="ignore")
                result = None
                failed = False
                bookcount += 1
                try:
                    result = USX.fromUsfm(sfmdat, sfmp, altparser=args.qusfm, timeout=args.timeout or 3600)
                except NoParseError as e:
                    imsg = " {} at {} in {}".format(e.msg, e.state.pos, f).replace("\r", "\\r")
                    results.append(imsg)
                    lognprint(f"{fpath}/{f}: {imsg}", args.quiet)
                    failed = True
                except TimeoutError:
                    lognprint(f"{fpath}: Timed out", args.quiet, logging.INFO)
                    results.append("Timed out")
                    failed = True
                except UnicodeError:
                    lognprint(f"{fpath}: Skipped. Encoding error, probably latin-1?", args.quiet, logging.INFO)
                    results.append("Encoding error")
                    failed = True
                if result is not None:
                    result.addesids()
                    etreexml = etree.fromstring(et.tostring(result.getroot(), encoding="utf-8", xml_declaration=True))
                    if not relax.validate(etreexml):
                        #xmsg = str(relax.error_log.last_error)
                        if failed:
                            results[-1] += f" |xml>"
                        else:
                            results.append(f"{fpath}/{f} |xml>")
                        lognprint(f"{fpath}/{f}: XML| generated xml invalid", args.quiet, logging.INFO)
                anyfailed |= failed
                if failed and args.oneerror:
                    break
            szip.close()
        if args.do in ("all", "usx"):
            usxjobs = [x for x in zipf.namelist() if x.startswith("release/USX_1/") and x.lower().endswith(".USX")]
            for ausx in usxjobs:
                with zipf.open(ausx) as xf:
                    doc = etree.parse(xf)
                    if not relax.validate(doc):
                        #xmsg = str(relax.error_log.last_error)
                        err = f"{fpath}/{ausx} XML, USX xml invalid"
                        results.append(err)
                        lognprint(err, args.quiet, logging.INFO)
                        anyfailed = True
                        if args.oneerror:
                            break
                    bookcount += 1
        zipf.close()
        if bookcount == 0 and not anyfailed:
            anyfailed = True
            results = [f"{fpath}: No files found"]
        lognprint("{}: {}".format(fpath, "Passed" if not anyfailed else "Failed"), args.quiet, logging.INFO)
        return (fpath, results)
    except Exception as e:
        lognprint(f"{fpath}/{f}: {e}", args.quiet, logging.INFO)
        lognprint("{}: {}".format(fpath, "Crashed"), args.quiet, logging.INFO)
        return (fpath, [str(e)])


parser = argparse.ArgumentParser()
parser.add_argument("directory",help="A single or tree of test directories")
parser.add_argument("-g","--grammar",required=True,help="Enhanced usx.rng RELAXng grammar")
parser.add_argument("-D","--do",default="all",help="Do all*, usx, usfm tests")
parser.add_argument("-S","--start",default="Scripture",help="Starting node for parsing")
parser.add_argument("-E","--extfiles",action='append',default=[],help='markers.ext files to include')
parser.add_argument("--qusfm",action="store_true",help="Use the faster non validating usfm parser")
parser.add_argument("-s","--size",type=float,default=500000,help="Max test file to run in kB")
parser.add_argument("-M","--match",help="Constrain input files to match regex")
parser.add_argument("--skipfile",help="Logfile name and skip all jobs skipped and passed in that file")
parser.add_argument("-O","--oneerror",action="store_true",help="Bail on a zip after the first faulty sfm file")
parser.add_argument("-T","--timeout",type=int,help="Maximum time to process a file in seconds")
parser.add_argument("-j","--jobs",type=int,default=0,help="Parallel jobs, single=1")
parser.add_argument("-C","--chunks",type=int,help="Size of parallel chunks")
parser.add_argument("-v","--verbose",action="store_true",help="Print things like error messages")
parser.add_argument("-q","--quiet",action="store_true",help="Don't report progress")
parser.add_argument("-l","--logging",default="error",help="Set logging level to usfmxtest.log")
parser.add_argument("--logfile",default="usfmtestdbl.log",help="Out log to file")
parser.add_argument("--logparse",action="store_true",help="Log the parsing as well")
parser.add_argument("-z","--debug",type=int,default=0,help="1=print tree, 2-alt multiproc")
args = parser.parse_args()

def chunkjobs(jobs, nchunks):
    chunksize = len(jobs) // nchunks
    chunksize += 1 if len(jobs) % nchunks else 0
    tjobs = [j[0] for j in sorted(jobs, key=lambda x:(-x[1], x[0]))]
    dojobs = []
    for i in range(nchunks):
        if args.debug & 2 == 0:
            lj = [x for x in itertools.chain(*itertools.zip_longest(
                    tjobs[i::2*nchunks], tjobs[2*nchunks-i-1::2*nchunks])) if x is not None]
        #dojobs.extend(x for x in itertools.chain(*zip(tjobs[i::2*nchunks]+[None], tjobs[2*nchunks-i-1::2*nchunks]+[None])))
        else:
            lj = tjobs[i::2*nchunks] + tjobs[2*nchunks-i-1::2*nchunks]
        dojobs.extend(lj + ([None] * (chunksize - len(lj))))
    sys.stderr.write(f"{len(dojobs)} jobs from {len(jobs)}, {chunksize=}, {nchunks=}\n".format(len(dojobs)))
    return dojobs

if args.logging:
    try:
        loglevel = int(args.logging)
    except ValueError:
        loglevel = getattr(logging, args.logging.upper(), None)
    if isinstance(loglevel, int):
        parms = {'level': loglevel, 'datefmt': '%d/%b/%Y %H:%M:%S',
                 'format': '%(asctime)s.%(msecs)03d %(levelname)s:%(module)s(%(lineno)d) %(message)s'}
        logfh = open(args.logfile, "w", encoding="utf-8")
        parms.update(stream=logfh, filemode="w") #, encoding="utf-8")
        try:
            logging.basicConfig(**parms)
        except FileNotFoundError as e:      # no write access to the log
            print("Exception", e)
    if not args.logparse:
        logging.config.dictConfig({'version': 1, 'disable_existing_loggers': True})
        if args.timeout is None:
            Parser.debug=False
    log = logging.getLogger('usfmtestdbl')

relaxns = "{http://relaxng.org/ns/structure/1.0}"

skips = {}
if args.skipfile and os.path.exists(args.skipfile):
    with open(args.skipfile, encoding="utf-8") as inf:
        for l in inf.readlines():
            m = re.match(r"^(.*): (\S+)(?:\s+\((Passed)\))?", l)
            if m is not None:
                if m.group(2) in ("Passed", "Skipped"):
                    skips[m.group(1)] = m.group(3) or m.group(2)

with open(args.grammar, "rb") as inf:
    rdoc = _grammarDoc(inf, args.extfiles, factory=etree)
sfmproc = _usfmGrammar(rdoc, start=args.start)

jobs = []
skipcount = 0
for dp, dns, fns in os.walk(args.directory):
    for f in fns:
        if not f.lower().endswith(".zip"):
            continue
        if args.match and not re.match(args.match, f, re.I):
            continue
        st = os.stat(os.path.join(dp, f))
        if st.st_size > args.size * 1024:
            continue
        jobname = os.path.join(dp, f)
        if jobname in skips:
            log.info(f"{jobname}: Skipped ({skips[jobname]})")
            skipcount += 1
        else:
            jobs.append((jobname, st.st_size))

if not args.quiet:
    print(f"Skipping {skipcount} jobs")

if args.jobs != 1 and len(jobs) > 1:
    set_start_method('fork')
    if args.jobs == 0:
        args.jobs = os.cpu_count()

    def initproc(newdoctxt, args, log):
        proc = current_process()
        newdoc = etree.fromstring(newdoctxt)
        rng =  etree.RelaxNG(etree=newdoc)
        proc.usxrng = rng
        proc.sfmproc = _usfmGrammar(rdoc, start=args.start)
        proc.log = log
        proc.args = args
    def doprocone(arg):
        if arg is None:
            return
        proc = current_process()
        res = proconezip(arg, proc.sfmproc, proc.log, proc.usxrng, proc.args)
        return (str(res[0]), [str(x) for x in res[1]])
    #pool = get_context('spawn').Pool()
    newdoctxt = et.tostring(rdoc.getroot())
    pool = Pool(args.jobs or None, initializer=initproc, initargs=(newdoctxt, args, log))
    if args.chunks:
        chunksize = args.chunks
        nchunks = len(jobs) // chunksize
    else:
        nchunks = args.jobs * 4
        chunksize = len(jobs) // nchunks
    dojobs = chunkjobs(jobs, nchunks)
    mapresults = pool.map_async(doprocone, dojobs, chunksize=chunksize)
    results = mapresults.get()
else:
    usxrng = etree.RelaxNG(etree=rdoc)
    results = []
    for j in jobs:
        results.append(proconezip(j[0], sfmproc, log, usxrng, args))

failed = 0
passed = 0
skipped = 0
for r in results:
    if r is None or r[1] is None:
        skipped += 1
    elif len(r[1]) == 0:
        passed += 1
    else:
        failed += 1
        if args.verbose:
            print("{}:\n    {}".format(r[0], "    \n".join(r[1])))
total = failed + passed + skipped
print(f"{passed=}, {failed=}, {skipped=}, {total=}")

