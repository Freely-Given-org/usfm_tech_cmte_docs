#!/usr/bin/env -S python3 -u

import argparse, os, sys, logging, re, itertools
from multiprocessing import Pool

try:
    from usfmtc.sfmparser import UsfmParserBackend, parseusfm
except ImportError:
    sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'lib'))
    from usfmtc.sfmparser import UsfmParserBackend, parseusfm
from usfmtc.parser import NoParseError
from usfmtc.grammar import UsfmGrammarParser, XmlGrammarParser
from usfmtc.usxgrammar import usxenums
from usfmtc.xmlutils import parsexml, writexml, prettyxml
from usfmtc.usx import addesids, etCmp, cell_aligns, cleanup, partypes
import xml.etree.ElementTree as et

def proconedir(d, sfmp, args):
    meta = {}
    doc = et.parse(os.path.join(d, "metadata.xml"))
    for e in doc.getroot():
        meta[e.tag] = e.text
    tags = set((meta.get('tags', "") or "").split())
    if len(args.exclude) and tags & args.exclude and (not len(tags) or not tags & args.include):
        return
    # print(f"{d}: ", end='', flush=True)
    imsg = ""
    try:
        result = parseusfm(os.path.join(d, "origin.usfm"), sfmp)
    except NoParseError as e:
        imsg = " {} at {}".format(e.msg, e.state.pos).replace("\r", "\\r")
        result = None
    if result is not None:
        if (args.debug & 1) != 0:
            print(result)
        xml = result.asEt()
        addesids(xml)
        cell_aligns(xml)
        cleanup(xml)
        if os.path.exists(os.path.join(d, "origin.xml")):
            testxml = parsexml(os.path.join(d, "origin.xml"))
            passed = etCmp(xml, testxml.getroot(), verbose=args.print)
        if args.print:
            prettyxml(xml)
            writexml(sys.stdout, xml)
    else:
        passed = False
    expected = meta.get('validated', 'pass') == 'pass'
    res = passed == expected
    print("{}: {} ({} {}){}".format(d, "Passed" if res else "Failed", "==" if res else "!=", "Pass" if expected else "Fail", imsg))
    return res

def chunkjobs(jobs, nchunks):
    chunksize = len(jobs) // nchunks
    chunksize += 1 if len(jobs) % nchunks else 0
    tjobs = [j[0] for j in sorted(jobs, key=lambda x:(-x[1], x[0]))]
    dojobs = []
    for i in range(nchunks):
        if args.debug & 2 == 0:
            lj = [x for x in itertools.chain(*itertools.zip_longest(
                    tjobs[i::2*nchunks], tjobs[2*nchunks-i-1::2*nchunks])) if x is not None]
        #dojobs.extend(x for x in itertools.chain(*zip(tjobs[i::2*nchunks]+[None], tjobs[2*nchunks-i-1::2*nchunks]+[None])))
        else:
            lj = tjobs[i::2*nchunks] + tjobs[2*nchunks-i-1::2*nchunks]
        dojobs.extend(lj + ([None] * (chunksize - len(lj))))
    sys.stderr.write(f"{len(dojobs)} jobs from {len(jobs)}, {chunksize=}, {nchunks=}\n".format(len(dojobs)))
    return dojobs

parser = argparse.ArgumentParser()
parser.add_argument("directory",help="A single or tree of test directories")
parser.add_argument("-g","--grammar",required=True,help="Enhanced usx.rng RELAXng grammar")
parser.add_argument("-S","--start",default="Scripture",help="Starting node for parsing")
parser.add_argument("-P","--print",action="store_true",help="Print generated xml")
parser.add_argument("-m","--marker",action='append',default=[],help='type=mkr,mka,mkb')
parser.add_argument("-j","--jobs",type=int,default=-1,help="Parallel jobs")
parser.add_argument("-x","--exclude",action="append",default=[],help="Exclude test if tag present")
parser.add_argument("-i","--include",action="append",default=[],help="Override excludes to include test if tag present")
parser.add_argument("-s","--size",type=float,default=500000,help="Max test file to run in kB")
parser.add_argument("-l","--logging",help="Set logging level to usfmxtest.log")
parser.add_argument("-z","--debug",type=int,default=0,help="1=print tree, 2-alt multiproc")
args = parser.parse_args()

if "clarify" not in args.include:
    args.exclude.append("clarify")
args.exclude = set(args.exclude)
args.include = set(args.include)

if args.logging:
    try:
        loglevel = int(args.logging)
    except ValueError:
        loglevel = getattr(logging, args.logging.upper(), None)
    if isinstance(loglevel, int):
        parms = {'level': loglevel, 'datefmt': '%d/%b/%Y %H:%M:%S', 'format': '%(asctime)s.%(msecs)03d %(levelname)s:%(module)s(%(lineno)d) %(message)s'}
        logfh = open("usfmxtest.log", "w", encoding="utf-8")
        parms.update(stream=logfh, filemode="w") #, encoding="utf-8")
        try:
            logging.basicConfig(**parms)
        except FileNotFoundError as e:      # no write access to the log
            print("Exception", e)
    log = logging.getLogger('usfmxtest')

jobs = []
for dp, dns, fns in os.walk(args.directory):
    if 'metadata.xml' in fns:
        st = os.stat(os.path.join(dp, "origin.usfm"))
        if st.st_size > args.size * 1024:
            continue
        jobs.append((dp, st.st_size))

if len(jobs) == 1:
    args.exclude = []

relaxns = "{http://relaxng.org/ns/structure/1.0}"

rdoc = et.parse(args.grammar)
if len(args.marker):
    for s in args.marker:
        t, r = s.split('=')
        mks = re.split(r'[,;\s]\s*', r)
        ty = t.strip()
        e = rdoc.find('./{0}define[@name="{1}.enum"]/{0}choice'.format(relaxns, usxenums[ty]))
        if e is None:
            continue
        for m in mks:
            v = et.Element('value')
            v.text = m.strip()
            e.insert(0, v)
            if ty == 'para':
                partypes[m] = 'Section'

backendsfm = UsfmParserBackend()
sfmproc = UsfmGrammarParser(rdoc, backendsfm)
sfmproc.parseRef(args.start)

if args.jobs > -1 and len(jobs) > 1:
    if args.jobs == 0:
        args.jobs = os.cpu_count()
    def doprocone(arg):
        if arg is None:
            return
        return proconedir(arg, sfmproc.curr, args)
    pool = Pool()
    nchunks = args.jobs * 4
    chunksize = len(jobs) // nchunks
    dojobs = chunkjobs(jobs, nchunks)
    results = pool.map_async(doprocone, dojobs, chunksize=chunksize)
    results.get()
else:
    for j in jobs:
        proconedir(j[0], sfmproc.curr, args)

